args:
    device: 'cuda:0'
    DEBUG: False
    #DEBUG: True
    transfer: True
    #transfer: False

    #results_dir: './VITT_TEST/'
    results_dir: './HOPEFUL_PUSHFORWARD_VIT_TEST/'

    pretrained_model_path: "./QUALITATIVE_PRETRAINED_MODELS/"

    model: 'pitt' 
    neural_operator: 'fno' # fno, oformer, deeponet

    ###
    # Training setup
    ###
    train_style: 'next_step'
    t_bundle: 1
    pushforward: 1
    batch_size: 32
    initial_step: 5
    num_seeds: 3
    
    ###
    # Choose data set
    ###
    base_path: '/home/cooperlorsung/' # Set to path of saved data
    #dataset: 'shallow_water'
    #dataset: 'diffusion_reaction'
    
    #dataset: 'cfd_rand_0.1_0.01_0.01'
    #dataset: 'cfd_rand_0.1_0.1_0.1'
    #dataset: 'cfd_rand_0.1_1e-8_1e-8'
    #dataset: 'cfd_rand_1.0_0.01_0.01'
    #dataset: 'cfd_rand_1.0_0.1_0.1'
    #dataset: 'cfd_rand_1.0_1e-8_1e-8'
    #dataset: 'cfd_turb_0.1_1e-8_1e-8'
    #dataset: 'cfd_turb_1.0_1e-8_1e-8'
    
    dataset: 'all'
    num_workers: 0
    reduced_resolution: 4
    reduced_resolution_t: 1
    reduced_batch: 1


    # Normalize across each channel of our data
    normalize: False
    #normalize: True

    clip: True
    pretraining_loss: 'clip'
    #pretraining_loss: 'weightedclip' # Or weightedclip or None


    ###
    # Choose level of sentence information
    ###
    llm: 'all-mpnet-base-v2'

    #bcs: False         # Whether or not we add boundary condition information into text description
    bcs: True           # Whether or not we add boundary condition information into text description

    # TODO: Explore adding coefficient information only to sentences, not additional channels
    #coeff: False        # Whether or not we use coefficient information
    coeff: True        # Whether or not we use coefficient information
    
    qualitative: False  # Whether or not we incorporate qualitative information into text description.
    #qualitative: True  # Whether or not we incorporate qualitative information into text description.

    sentence: False     # Whether or not we're returning sentences and training LLM end-to-end
    #sentence: True     # Whether or not we're returning sentences and training LLM end-to-end
    
    # Training Optimizer
    epochs: 1000
    learning_rate: 1.e-3
    weight_decay: 1.e-8
    sim_time: 21
    num_samples: 1000

    # Pretraining Optimizer
    pretraining_num_samples: 0
    pretraining_epochs: 100
    pretraining_learning_rate: 1.e-3
    pretraining_weight_decay: 1.e-9

    # Tracking
    validate: 1
    log_freq: 25
    progress_plot_freq: 100

    ###
    # Model hyperparameters
    ###
    # ViT Parameters
    #downsample: 1
    #img_size: 128
    img_size: 64
    #img_size: 32
    patch_size: 8
    patch_stride: 8 # Works if stride >= patch_size/2? Not exactly sure...
    dim: 64
    depth: 3
    heads: 8
    mlp_dim: 128
    pool: 'mean'
    dim_head: 64
    dropout: 0.01
    emb_dropout: 0.01

    qkv_bias: True

