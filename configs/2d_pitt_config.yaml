args:
    if_training: True
    neural_operator: False
    #results_dir: './2D_ns_next_step/'
    #results_dir: './new_CLIP_FF_IC_20/'
    results_dir: './new_large_diff_embed/'

    model: 'pitt' 
    neural_operator: 'fno' # fno, oformer, deeponet

    embedding: 'standard'
    #embedding: 'novel'
    #

    #split_style: 'equation'
    split_style: 'initial_condition'

    train_style: 'next_step'
    #train_style: 'fixed_future'
    #
    
    ###
    # Choose data set
    ###
    #dataset: 'shallow_water'
    #dataset: 'diffusion_reaction'
    #dataset: 'cfd_rand_0.1_0.01_0.01'
    #dataset: 'cfd_rand_0.1_0.1_0.1'
    #dataset: 'cfd_rand_0.1_1e-8_1e-8'
    #dataset: 'cfd_rand_1.0_0.01_0.01'
    #dataset: 'cfd_rand_1.0_0.1_0.1'
    #dataset: 'cfd_rand_1.0_1e-8_1e-8'
    #dataset: 'cfd_turb_0.1_1e-8_1e-8'
    #dataset: 'cfd_turb_1.0_1e-8_1e-8'
    dataset: 'all'

    ###
    # Choose embedding strategy
    ###

    clip: True
    coeff: False       # Whether or not we use coefficient information
    #coeff: True       # Whether or not we use coefficient information

    # Whether or not we add boundary condition information into text description
    #bcs: False
    bcs: True

    # Whether or not we incorporate qualitative information into text description. Can't use this without coefficient information
    qualitative: False
    #qualitative: True

    # Whether or not we're returning sentences and training LLM end-to-end
    sentence: False
    #sentence: True
    transfer: False

    pretraining_loss: 'clip'
    #pretraining_loss: 'weightedclip' # Or weightedclip or None
    llm: 'None'
    
    attention_type: 'galerkin'

    base_path: '/home/cooperlorsung/' # Set to path of saved data
    continue_training: False
    augment: False
    forcing: False
    rollout_length: 1
    num_workers: 0
    batch_size: 16
    initial_step: 10
    #initial_step: 41
    t_train: 200
    model_update: 1

    # Data file
    #data_name: '2d_ns_1s_256_4eq.h5'
    data_name: '2d_ns_30s_256_370eq.h5'
    #data_name: '2d_electric_100_60.h5'
    
    fno: False
    return_text: True
    reduced_resolution: 4
    reduced_resolution_t: 1
    reduced_batch: 1

    epochs: 200
    num_seeds: 5
    dropout: 0.6

    # Optimizer
    learning_rate: 1.e-3
    weight_decay: 1.e-4
    scheduler_step: 2
    scheduler_gamma: 0.1
    training_type: single

    #
    #num_x: 60
    #num_y: 100
    num_x: 64
    num_y: 64
    num_t: 100

    # Fixed Future
    sim_time: 80
    #sim_time: 120
    
    # Next-step
    #sim_time: 1000

    #num_samples: 10
    #num_samples: 100
    #num_samples: 370
    num_samples: 1000

    # Pretraining optimizer
    pretraining_num_samples: 0
    #pretraining_num_samples: 10
    #pretraining_num_samples: 100
    #pretraining_num_samples: 1000
    #pretraining_num_samples: 10000
    #pretraining_epochs: 1
    pretraining_epochs: 100
    pretraining_learning_rate: 1.e-3
    pretraining_weight_decay: 1.e-9
    pretraining_training_type: single
    
    # For mixed next step training
    #samples_per_equation: 1
    samples_per_equation: 1

    # Transformer
    hidden: 64
    layers: 4
    heads: 4
    
    # FNO
    modes1: 8
    modes2: 8
    width: 64
    num_channels: 1

    # DeepONet
    branch_net: [10, 128, 128]
    trunk_net: [2, 128, 128]
    activation: 'silu'
    kernel_initializer: 'Glorot normal'

    # OFormer
    # Encoder
    input_channels: 12    # Number of frames?
    #input_channels: 43    # Number of frames?
    heads: 4
    in_emb_dim: 64       # num_x
    out_seq_emb_dim: 64  # Embedding
    depth: 2              # Number of layers
    #enc_res: 2048             # Not sure
    enc_res: 64             # Not sure

    # Decoder
    latent_channels: 64  # 256??
    out_channels: 1       # 1D output
    decoder_depth: 1     # Number of layers
    scale: 1              # Not sure
    #dec_res: 2048             # Not sure
    dec_res: 128             # Not sure


    # Tracking
    log_freq: 5
    progress_plot_freq: 10
