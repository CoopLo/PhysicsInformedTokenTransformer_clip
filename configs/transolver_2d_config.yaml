args:
    device: 'cuda:0'
    #results_dir: './pdebench_vit_results/'
    #results_dir: './combined_pdebench_vit_results/'
    #results_dir: './overlap_combined_pdebench_transolver_results/'
    #results_dir: './test_transfer/'
    results_dir: './leterrip_results_transfer/'

    base_path: '/home/cooperlorsung/'
    #pretrained_model_path: "./AS_PRETRAINED_VITS/"
    #pretrained_model_path: "./NOPRETRAIN_AS_PRETRAINED_VITS/"
    #pretrained_model_path: "./FULL_PRETRAIN_AS_PRETRAINED_VITS/"
    pretrained_model_path: "./QUALITATIVE_PRETRAINED_MODELS/"

    # Toggle moddel loading
    load_pretrained: True
    #load_pretrained: False

    train_style: 'next_step'
    #train_style: 'fixed_future'
    #train_style: 'arbitrary_step'
    #

    ###
    # Choose data set
    ###
    #dataset: 'shallow_water'
    #dataset: 'diffusion_reaction'
    #dataset: 'cfd_rand_0.1_0.01_0.01'
    #dataset: 'cfd_rand_0.1_0.1_0.1'
    #dataset: 'cfd_rand_0.1_1e-8_1e-8'
    #dataset: 'cfd_rand_1.0_0.01_0.01'
    #dataset: 'cfd_rand_1.0_0.1_0.1'
    #dataset: 'cfd_rand_1.0_1e-8_1e-8'
    #dataset: 'cfd_turb_0.1_1e-8_1e-8'
    #dataset: 'cfd_turb_1.0_1e-8_1e-8'
    dataset: 'all'

    ###
    # Choose embedding strategy
    ###
    
    clip: True
    coeff: False       # Whether or not we use coefficient information
    #coeff: True       # Whether or not we use coefficient information
    
    # Whether or not we add boundary condition information into text description
    #bcs: False
    bcs: True
    
    # Whether or not we incorporate qualitative information into text description. Can't use this without coefficient information
    qualitative: False
    #qualitative: True

    # Whether or not we're returning sentences and training LLM end-to-end
    sentence: False 
    #sentence: True

    #######################

    ###
    #  Choose pretraining/finetuning strategy
    ###
    pretraining: 'clip' # Or transfer...
    transfer: True
    #transfer: False

    pretraining_loss: 'clip'
    #pretraining_loss: 'weightedclip' # Or weightedclip or None
    #llm: 'all-MiniLM-L6-v2'
    llm: 'all-mpnet-base-v2'

    num_workers: 0
    #batch_size: 16
    #batch_size: 32
    batch_size: 32
    #batch_size: 128
    #pretraining_batch_size: 32
    pretraining_batch_size: 256
    #pretraining_batch_size: 512
    #initial_step: 1
    initial_step: 5
    #initial_step: 7

    t_train: 200
    validate: 100

    reduced_resolution: 2
    reduced_resolution_t: 1
    reduced_batch: 1

    # Optimizer
    #num_samples: 100
    num_samples: 500
    learning_rate: 1.e-3
    weight_decay: 1.e-8
    scheduler_step: 100
    scheduler_gamma: 0.5
    #epochs: 1000
    #epochs: 1
    #epochs: 5
    epochs: 500
    #epochs: 2000
    num_seeds: 1

    # Pretraining optimizer
    pretraining_num_samples: 0
    #pretraining_num_samples: 10
    #pretraining_num_samples: 100
    #pretraining_num_samples: 1000
    #pretraining_num_samples: 10000
    #pretraining_epochs: 1
    pretraining_epochs: 100
    pretraining_learning_rate: 1.e-3
    pretraining_weight_decay: 1.e-9
    pretraining_training_type: single

    # ViT Parameters
    img_size: 128    # Note: Setting img_size overrides reduced_resolution
    #img_size: 64
    #img_size: 32

    patch_size: 8
    patch_stride: 8 # Works if stride >= patch_size/2? Not exactly sure...
    dim: 64
    depth: 3
    heads: 8
    mlp_dim: 128
    pool: 'mean'
    dim_head: 64
    dropout: 0.01
    emb_dropout: 0.01

    qkv_bias: True

    #num_x: 64
    #num_y: 64
    #num_x: 60
    #num_y: 100
    
    sim_time: 21

    # Set to 0 to skip pretraining
    #pretraining_num_samples: 0
    samples_per_equation: 1

    # Tracking
    log_freq: 100
    progress_plot_freq: 100
